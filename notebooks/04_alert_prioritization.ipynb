{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Alert Prioritization: From Detection to Decision\n",
    "\n",
    "**Objective**: Transform raw anomaly scores into actionable SOC workflows.\n",
    "\n",
    "This notebook extends BSAD beyond detection into **post-detection triage**:\n",
    "- Risk scoring with uncertainty and entity context\n",
    "- Alert budget calibration\n",
    "- Operational metrics (alerts/1k, FPR@fixed recall)\n",
    "- Entity-enriched alert tickets\n",
    "\n",
    "---\n",
    "\n",
    "## The Problem: Alert Fatigue\n",
    "\n",
    "Detection is only half the battle. A SOC analyst faces:\n",
    "- Thousands of alerts per day\n",
    "- Limited time to investigate each\n",
    "- Need to prioritize: which alerts matter most?\n",
    "\n",
    "**This notebook answers**: Given a fixed analyst capacity, how do we maximize attack detection while minimizing false positives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "from triage import (\n",
    "    compute_risk_score,\n",
    "    RiskScorer,\n",
    "    calibrate_threshold,\n",
    "    AlertBudget,\n",
    "    build_alert_budget_curve,\n",
    "    precision_at_k,\n",
    "    recall_at_k,\n",
    "    fpr_at_fixed_recall,\n",
    "    alerts_per_k_windows,\n",
    "    workload_reduction,\n",
    "    ranking_report,\n",
    "    build_entity_history,\n",
    "    enrich_alerts,\n",
    ")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Detection Results\n",
    "\n",
    "We'll use the multi-regime comparison results from CSE-CIC-IDS2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multi-regime results\n",
    "results_path = Path.cwd().parent / \"outputs\" / \"datasets\" / \"cse-cic-ids2018\" / \"multi-regime\" / \"multi_regime_results.json\"\n",
    "\n",
    "with open(results_path) as f:\n",
    "    multi_regime = json.load(f)\n",
    "\n",
    "print(\"Regimes analyzed:\")\n",
    "for regime in multi_regime['regimes']:\n",
    "    print(f\"  - {regime['target_rate']*100:.0f}% attack rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also load UNSW scored data for detailed analysis\n",
    "unsw_path = Path.cwd().parent / \"outputs\" / \"datasets\" / \"unsw-nb15\" / \"rare-attack\" / \"scored_df_2pct.parquet\"\n",
    "\n",
    "if unsw_path.exists():\n",
    "    df = pd.read_parquet(unsw_path)\n",
    "    print(f\"UNSW-NB15 Rare Attack (2%)\")\n",
    "    print(f\"  Observations: {len(df):,}\")\n",
    "    print(f\"  Attack rate: {df['has_attack'].mean():.2%}\")\n",
    "    print(f\"  Columns: {df.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"UNSW data not found. Run train_rare_attack_model.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Risk Score Formula\n",
    "\n",
    "Raw anomaly scores are not actionable. We need a **composite risk score** that incorporates:\n",
    "\n",
    "```\n",
    "Risk = w1 * normalize(anomaly_score) \n",
    "     + w2 * confidence(1/uncertainty)\n",
    "     + w3 * novelty(entity_history)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `anomaly_score`: How unusual is this observation?\n",
    "- `confidence`: How certain are we about the score?\n",
    "- `novelty`: Is this entity new (less history = higher risk)?\n",
    "\n",
    "This is configurable through weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate risk score computation\n",
    "if 'df' in dir():\n",
    "    # Default weights: (anomaly=0.5, confidence=0.3, novelty=0.2)\n",
    "    risk_scores = compute_risk_score(\n",
    "        df,\n",
    "        score_col=\"anomaly_score\",\n",
    "        std_col=\"score_std\" if \"score_std\" in df.columns else None,\n",
    "        entity_col=\"entity\" if \"entity\" in df.columns else None,\n",
    "    )\n",
    "    \n",
    "    df[\"risk_score\"] = risk_scores\n",
    "    \n",
    "    print(\"Risk Score Statistics:\")\n",
    "    print(df[\"risk_score\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk score vs anomaly score\n",
    "if 'df' in dir():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Scatter: anomaly vs risk\n",
    "    ax1 = axes[0]\n",
    "    colors = ['crimson' if x else 'steelblue' for x in df['has_attack']]\n",
    "    ax1.scatter(df['anomaly_score'], df['risk_score'], c=colors, alpha=0.5, s=20)\n",
    "    ax1.set_xlabel('Anomaly Score (raw)')\n",
    "    ax1.set_ylabel('Risk Score (composite)')\n",
    "    ax1.set_title('Anomaly Score vs Risk Score\\n(Red = attack, Blue = normal)')\n",
    "    \n",
    "    # Distribution comparison\n",
    "    ax2 = axes[1]\n",
    "    attacks = df[df['has_attack'] == 1]['risk_score']\n",
    "    benign = df[df['has_attack'] == 0]['risk_score']\n",
    "    ax2.hist(benign, bins=30, alpha=0.6, label=f'Normal (n={len(benign)})', color='steelblue', density=True)\n",
    "    ax2.hist(attacks, bins=30, alpha=0.6, label=f'Attack (n={len(attacks)})', color='crimson', density=True)\n",
    "    ax2.set_xlabel('Risk Score')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_title('Risk Score Distribution by Class')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Alert Budget Calibration\n",
    "\n",
    "SOCs have limited capacity. Instead of asking \"what's the best threshold?\", we ask:\n",
    "\n",
    "> **\"If I can only review X alerts per day, what recall can I achieve?\"**\n",
    "\n",
    "This is the **alert budget** approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    y_true = df['has_attack'].astype(int).values\n",
    "    scores = df['anomaly_score'].values\n",
    "    \n",
    "    # Build alert budget curve\n",
    "    budget_curve = build_alert_budget_curve(scores, y_true)\n",
    "    \n",
    "    print(\"Alert Budget Curve:\")\n",
    "    print(budget_curve[['recall_target', 'actual_recall', 'fpr', 'alerts']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Alert budget curve\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(budget_curve['actual_recall'] * 100, budget_curve['alerts'], 'o-', \n",
    "             color='#2ecc71', linewidth=2, markersize=10)\n",
    "    ax1.set_xlabel('Recall (%)', fontsize=12)\n",
    "    ax1.set_ylabel('Total Alerts', fontsize=12)\n",
    "    ax1.set_title('Alert Budget Curve\\n\"How many alerts to catch X% of attacks?\"', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate key points\n",
    "    for _, row in budget_curve.iterrows():\n",
    "        if row['recall_target'] in [0.3, 0.5, 0.7]:\n",
    "            ax1.annotate(f\"{int(row['alerts'])} alerts\\n@{int(row['actual_recall']*100)}% recall\",\n",
    "                        xy=(row['actual_recall']*100, row['alerts']),\n",
    "                        xytext=(10, 10), textcoords='offset points',\n",
    "                        fontsize=10, ha='left',\n",
    "                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # FPR at fixed recall\n",
    "    ax2 = axes[1]\n",
    "    ax2.bar(budget_curve['recall_target'] * 100, budget_curve['fpr'], \n",
    "            width=8, color='#9b59b6', alpha=0.7)\n",
    "    ax2.set_xlabel('Target Recall (%)', fontsize=12)\n",
    "    ax2.set_ylabel('False Positive Rate', fontsize=12)\n",
    "    ax2.set_title('Cost of Detection\\n\"FPR required to achieve X% recall\"', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ranking Metrics: Precision@k and Recall@k\n",
    "\n",
    "For SOC analysts, what matters is:\n",
    "- **Precision@k**: \"Of my top k alerts, how many are real attacks?\"\n",
    "- **Recall@k**: \"What fraction of all attacks are in my top k?\"\n",
    "\n",
    "These are more actionable than ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    # Generate ranking report\n",
    "    report = ranking_report(y_true, scores)\n",
    "    \n",
    "    print(\"Ranking Metrics Report:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(report.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir():\n",
    "    # Visualize precision@k and recall@k\n",
    "    ks = [5, 10, 25, 50, 100]\n",
    "    prec = [precision_at_k(y_true, scores, k) for k in ks]\n",
    "    rec = [recall_at_k(y_true, scores, k) for k in ks]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(ks))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, prec, width, label='Precision@k', color='#3498db')\n",
    "    bars2 = ax.bar(x + width/2, rec, width, label='Recall@k', color='#e74c3c')\n",
    "    \n",
    "    ax.set_xlabel('k (number of top alerts)', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Precision and Recall at Top-k\\n\"Quality of the ranked alert list\"', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(ks)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, prec):\n",
    "        ax.annotate(f'{val:.2f}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                   xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n",
    "    for bar, val in zip(bars2, rec):\n",
    "        ax.annotate(f'{val:.2f}', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                   xytext=(0, 3), textcoords='offset points', ha='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Regime Comparison: Operational Metrics\n",
    "\n",
    "Let's compare BSAD vs Random Forest across attack rate regimes using **operational metrics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison table from multi-regime results\n",
    "comparison_data = []\n",
    "\n",
    "for regime in multi_regime['regimes']:\n",
    "    rate = regime['target_rate']\n",
    "    \n",
    "    for model in ['BSAD', 'RandomForest']:\n",
    "        metrics = regime['metrics'][model]\n",
    "        comparison_data.append({\n",
    "            'Attack Rate': f\"{rate*100:.0f}%\",\n",
    "            'Model': model,\n",
    "            'ROC-AUC': metrics['roc_auc'],\n",
    "            'FPR@R=0.3': metrics['fpr_at_recall_30'],\n",
    "            'Alerts/1k': metrics['alerts_per_1k_windows'],\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"Multi-Regime Comparison: BSAD vs Random Forest\")\n",
    "print(\"=\" * 60)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the key insight: alert volume reduction\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "rates = [r['target_rate'] * 100 for r in multi_regime['regimes']]\n",
    "bsad_alerts = [r['metrics']['BSAD']['alerts_per_1k_windows'] for r in multi_regime['regimes']]\n",
    "rf_alerts = [r['metrics']['RandomForest']['alerts_per_1k_windows'] for r in multi_regime['regimes']]\n",
    "\n",
    "# Alert volume comparison\n",
    "ax1 = axes[0]\n",
    "ax1.plot(rates, bsad_alerts, 'o-', label='BSAD', color='#2ecc71', linewidth=2, markersize=10)\n",
    "ax1.plot(rates, rf_alerts, 's-', label='Random Forest', color='#e74c3c', linewidth=2, markersize=10)\n",
    "ax1.set_xlabel('Attack Rate (%)', fontsize=12)\n",
    "ax1.set_ylabel('Alerts per 1,000 Windows', fontsize=12)\n",
    "ax1.set_title('Alert Volume at Fixed Recall (30%)\\n\"Lower is better for SOC\"', fontsize=14)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.invert_xaxis()  # Rare events on right\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Reduction factor\n",
    "ax2 = axes[1]\n",
    "reduction = [rf/bsad if bsad > 0 else 0 for rf, bsad in zip(rf_alerts, bsad_alerts)]\n",
    "colors = ['#2ecc71' if r > 1 else '#e74c3c' for r in reduction]\n",
    "bars = ax2.bar([f\"{r:.0f}%\" for r in rates], reduction, color=colors, alpha=0.7)\n",
    "ax2.axhline(y=1, color='black', linestyle='--', linewidth=1)\n",
    "ax2.set_xlabel('Attack Rate', fontsize=12)\n",
    "ax2.set_ylabel('Alert Reduction Factor (RF/BSAD)', fontsize=12)\n",
    "ax2.set_title('BSAD Alert Reduction vs Random Forest\\n\"Higher = BSAD generates fewer alerts\"', fontsize=14)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, reduction):\n",
    "    ax2.annotate(f'{val:.1f}×', xy=(bar.get_x() + bar.get_width()/2, val),\n",
    "                xytext=(0, 5), textcoords='offset points', ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entity Context: Enriched Alert Tickets\n",
    "\n",
    "Analysts need **context**, not just scores. For each alert, we provide:\n",
    "- Entity baseline behavior\n",
    "- Deviation from baseline (σ)\n",
    "- Historical alert count\n",
    "- Confidence level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in dir() and 'entity' in df.columns and 'event_count' in df.columns:\n",
    "    # Build entity history\n",
    "    history = build_entity_history(df, entity_col='entity', value_col='event_count')\n",
    "    \n",
    "    # Enrich top alerts\n",
    "    enriched = enrich_alerts(df, history, top_k=10)\n",
    "    \n",
    "    print(\"Top 10 Enriched Alert Tickets:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for i, alert in enumerate(enriched, 1):\n",
    "        print(f\"\\n[Ticket #{i}]\")\n",
    "        print(f\"  Entity: {alert['entity_id']}\")\n",
    "        print(f\"  Anomaly Score: {alert['anomaly_score']:.2f}\")\n",
    "        print(f\"  Deviation: {alert['sigma_deviation']:.1f}σ from baseline\")\n",
    "        print(f\"  Baseline: {alert['baseline_mean']:.1f} ± {alert['baseline_std']:.1f}\")\n",
    "        print(f\"  Current Value: {alert['current_value']:.1f}\")\n",
    "        print(f\"  Confidence: {alert['confidence']}\")\n",
    "        print(f\"  Prior Alerts: {alert['historical_alerts']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "### The Operational Value of BSAD\n",
    "\n",
    "| Metric | Meaning | BSAD Advantage |\n",
    "|--------|---------|----------------|\n",
    "| **Alerts/1k** | Workload per analyst | 8-14× fewer alerts |\n",
    "| **FPR@R=0.3** | Cost of 30% recall | Up to 92% lower |\n",
    "| **Precision@k** | Quality of top alerts | Comparable |\n",
    "| **Entity context** | Analyst decision support | Built-in |\n",
    "\n",
    "### When to Use This Approach\n",
    "\n",
    "✅ **Use alert prioritization when**:\n",
    "- SOC has limited analyst capacity\n",
    "- False positives cause alert fatigue\n",
    "- Need to justify detection thresholds\n",
    "- Attacks are rare (<5%)\n",
    "\n",
    "❌ **Don't use when**:\n",
    "- Every alert must be reviewed (compliance)\n",
    "- Real-time blocking required\n",
    "- Attack rate is high (use classification)\n",
    "\n",
    "### The Bottom Line\n",
    "\n",
    "> **Detection systems should be evaluated not only by how well they separate classes, but by how well they manage human attention under uncertainty.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reproduce with One Command\n",
    "\n",
    "All results in this notebook can be reproduced with:\n",
    "\n",
    "```bash\n",
    "python scripts/alert_prioritization.py\n",
    "```\n",
    "\n",
    "Outputs saved to `outputs/triage/`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
