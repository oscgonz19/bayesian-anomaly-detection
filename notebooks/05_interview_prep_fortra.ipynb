{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BSAD - PreparaciÃ³n para Entrevista TÃ©cnica\n",
    "\n",
    "**Proyecto:** Bayesian Security Anomaly Detection  \n",
    "**Objetivo:** DetecciÃ³n de anomalÃ­as en datos de seguridad usando modelado Bayesiano jerÃ¡rquico\n",
    "\n",
    "---\n",
    "\n",
    "## Ãndice\n",
    "\n",
    "1. [Stack TÃ©cnico y LibrerÃ­as](#1-stack-tÃ©cnico)\n",
    "2. [El Problema que Resuelve BSAD](#2-el-problema)\n",
    "3. [Modelo MatemÃ¡tico](#3-modelo-matemÃ¡tico)\n",
    "4. [Pipeline End-to-End](#4-pipeline)\n",
    "5. [MÃ©tricas y EvaluaciÃ³n](#5-mÃ©tricas)\n",
    "6. [MÃ³dulo Triage (Operacional)](#6-triage)\n",
    "7. [Preguntas de Entrevista](#7-preguntas)\n",
    "8. [CÃ³digo Ejecutable](#8-cÃ³digo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Stack TÃ©cnico y LibrerÃ­as <a id='1-stack-tÃ©cnico'></a>\n",
    "\n",
    "### Core Bayesiano\n",
    "| LibrerÃ­a | VersiÃ³n | PropÃ³sito |\n",
    "|----------|---------|----------|\n",
    "| **PyMC** | 5.10+ | Framework de programaciÃ³n probabilÃ­stica. Usa MCMC (HMC/NUTS) para inferencia |\n",
    "| **ArviZ** | 0.17+ | DiagnÃ³sticos MCMC, visualizaciÃ³n de posteriors, formato InferenceData |\n",
    "| **PyTensor** | 2.18+ | Backend de cÃ³mputo tensorial (reemplaza Theano) |\n",
    "\n",
    "### Data Science\n",
    "| LibrerÃ­a | PropÃ³sito |\n",
    "|----------|----------|\n",
    "| **NumPy** | Arrays numÃ©ricos, operaciones vectorizadas |\n",
    "| **Pandas** | DataFrames, manipulaciÃ³n de datos tabulares |\n",
    "| **SciPy** | Distribuciones estadÃ­sticas (NegBinomial), funciones especiales (logsumexp) |\n",
    "| **Scikit-learn** | MÃ©tricas (PR-AUC, ROC-AUC), train/test split |\n",
    "\n",
    "### VisualizaciÃ³n & CLI\n",
    "| LibrerÃ­a | PropÃ³sito |\n",
    "|----------|----------|\n",
    "| **Matplotlib/Seaborn** | GrÃ¡ficos estÃ¡ticos |\n",
    "| **Typer** | CLI framework (comandos `bsad train`, `bsad score`, etc.) |\n",
    "| **Rich** | Formateo de tablas en terminal |\n",
    "\n",
    "### I/O\n",
    "| LibrerÃ­a | PropÃ³sito |\n",
    "|----------|----------|\n",
    "| **PyArrow** | Lectura/escritura eficiente de Parquet |\n",
    "| **NetCDF4** | Almacenamiento de traces MCMC (via ArviZ) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. El Problema que Resuelve BSAD <a id='2-el-problema'></a>\n",
    "\n",
    "### CaracterÃ­sticas de los datos de seguridad\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  DATOS DE SEGURIDAD tÃ­picos:                                â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  â€¢ Conteos (enteros): intentos de login, requests, bytes   â”‚\n",
    "â”‚  â€¢ Estructura entidad-tiempo: user_id Ã— dÃ­a                â”‚\n",
    "â”‚  â€¢ Eventos RAROS: <5% son ataques (class imbalance extremo)â”‚\n",
    "â”‚  â€¢ Overdispersion: Varianza >> Media                       â”‚\n",
    "â”‚  â€¢ Entidades heterogÃ©neas: algunos users muy activos,      â”‚\n",
    "â”‚    otros casi inactivos                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Por quÃ© los mÃ©todos clÃ¡sicos fallan\n",
    "\n",
    "| MÃ©todo | Problema con datos de seguridad |\n",
    "|--------|--------------------------------|\n",
    "| **Z-score global** | Ignora que cada entidad tiene su propio baseline |\n",
    "| **Poisson** | Asume var = mean; security data tiene var >> mean |\n",
    "| **Random Forest / XGBoost** | No da incertidumbre, requiere feature engineering manual, sufre con class imbalance |\n",
    "| **Isolation Forest** | No modela estructura entidad-tiempo |\n",
    "\n",
    "### La soluciÃ³n BSAD\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  BSAD usa:                                                  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  1. Negative Binomial â†’ maneja overdispersion              â”‚\n",
    "â”‚  2. Hierarchical model â†’ cada entidad tiene su Î¸[e]        â”‚\n",
    "â”‚  3. Partial pooling â†’ entidades sparse \"toman prestado\"    â”‚\n",
    "â”‚     de la poblaciÃ³n                                        â”‚\n",
    "â”‚  4. Full Bayesian â†’ incertidumbre cuantificada             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Modelo MatemÃ¡tico <a id='3-modelo-matemÃ¡tico'></a>\n",
    "\n",
    "### Estructura JerÃ¡rquica (3 niveles)\n",
    "\n",
    "```\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  NIVEL 1: Hiperpriors Poblacionales                         â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â•‘\n",
    "â•‘  Î¼ ~ Exponential(Î» = 0.1)    # Tasa media global            â•‘\n",
    "â•‘  Î± ~ HalfNormal(Ïƒ = 2.0)     # Shape/overdispersion         â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  NIVEL 2: Tasas por Entidad (Partial Pooling)               â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â•‘\n",
    "â•‘  Î¸[e] ~ Gamma(shape=Î±Â·Î¼, rate=Î±)  para cada entidad e      â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘  â†’ E[Î¸[e]] = Î¼  (centrado en media poblacional)             â•‘\n",
    "â•‘  â†’ Var[Î¸[e]] = Î¼/Î± (controlado por Î±)                       â•‘\n",
    "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "â•‘  NIVEL 3: Observaciones                                      â•‘\n",
    "â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â•‘\n",
    "â•‘  Ï† ~ HalfNormal(Ïƒ = 2.0)     # DispersiÃ³n observacional     â•‘\n",
    "â•‘  y[i] ~ NegBinomial(Î¼=Î¸[entity[i]], Î±=Ï†)                    â•‘\n",
    "â•‘                                                              â•‘\n",
    "â•‘  donde: Var(y) = Î¼ + Î¼Â²/Ï†  (overdispersion cuando Ï† < âˆ)    â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "```\n",
    "\n",
    "### Diagrama Visual\n",
    "\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚  Î¼, Î±     â”‚  â† HiperparÃ¡metros poblacionales\n",
    "                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜\n",
    "                          â”‚\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â–¼               â–¼               â–¼\n",
    "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”\n",
    "      â”‚ Î¸[1]  â”‚       â”‚ Î¸[2]  â”‚  ...  â”‚ Î¸[E]  â”‚  â† Tasas por entidad\n",
    "      â””â”€â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”€â”¬â”€â”€â”€â”˜       â””â”€â”€â”€â”¬â”€â”€â”€â”˜\n",
    "          â”‚               â”‚               â”‚\n",
    "    â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”\n",
    "    â–¼     â–¼     â–¼   â–¼     â–¼     â–¼   â–¼     â–¼     â–¼\n",
    "  y[1,1] y[1,2]... y[2,1] y[2,2]... y[E,1] y[E,2]...  â† Observaciones\n",
    "```\n",
    "\n",
    "### Partial Pooling: La Magia\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                â”‚\n",
    "â”‚  NO POOLING          PARTIAL POOLING         COMPLETE POOLING â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚\n",
    "â”‚  Cada entidad        Balance automÃ¡tico      Todas las        â”‚\n",
    "â”‚  independiente       segÃºn datos             entidades igualesâ”‚\n",
    "â”‚                                                                â”‚\n",
    "â”‚  Î¸[e] estimado       Î¸[e] = weighted avg     Î¸[e] = Î¼ global  â”‚\n",
    "â”‚  solo con datos      de datos propios +      para todos       â”‚\n",
    "â”‚  de entidad e        prior poblacional                        â”‚\n",
    "â”‚                                                                â”‚\n",
    "â”‚  PROBLEMA:           SOLUCIÃ“N BSAD:          PROBLEMA:        â”‚\n",
    "â”‚  Alta varianza       Shrinkage adaptativo    Alto sesgo       â”‚\n",
    "â”‚  para sparse         hacia Î¼                 ignora diferenciasâ”‚\n",
    "â”‚                                                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Anomaly Score\n",
    "\n",
    "```python\n",
    "# Para cada observaciÃ³n y[i]:\n",
    "anomaly_score[i] = -log P(y[i] | posterior)\n",
    "\n",
    "# Donde P(y|posterior) se calcula promediando sobre muestras MCMC:\n",
    "log P(y) = logsumexp([log P(y | Î¸_s, Ï†_s) for s in samples]) - log(n_samples)\n",
    "```\n",
    "\n",
    "**InterpretaciÃ³n:**\n",
    "- Score ALTO = probabilidad BAJA bajo el modelo = ANÃ“MALO\n",
    "- Score BAJO = probabilidad ALTA = comportamiento normal"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 3.5 Diagrama de Arquitectura Completa <a id='3.5-arquitectura'></a>\n\n```mermaid\nflowchart TB\n    subgraph DATA[\"ğŸ“Š 1. DATOS\"]\n        RAW[(\"Eventos Crudos<br/>user_id, timestamp,<br/>event_count\")]\n        VALID[\"Validar:<br/>âœ“ Conteos<br/>âœ“ Entidades<br/>âœ“ Var >> Mean<br/>âœ“ <5% ataques\"]\n        RAW --> VALID\n    end\n\n    subgraph FEATURES[\"âš™ï¸ 2. FEATURES\"]\n        AGG[\"GROUP BY<br/>(entidad, ventana)\"]\n        SPLIT[\"Split Temporal<br/>Train | Test\"]\n        AGG --> SPLIT\n    end\n\n    subgraph MODEL[\"ğŸ§  3. MODELO JERÃRQUICO\"]\n        direction TB\n        POP[\"ğŸŒ PoblaciÃ³n<br/>Î¼, Î±\"]\n        ENT[\"ğŸ‘¥ Entidad<br/>Î¸[e] ~ Gamma(Î¼Î±, Î±)\"]\n        OBS[\"ğŸ“ˆ ObservaciÃ³n<br/>y ~ NegBin(Î¸[e], Ï†)\"]\n        POP -->|\"Partial Pooling\"| ENT --> OBS\n    end\n\n    subgraph SCORE[\"ğŸ“Š 4. SCORING\"]\n        CALC[\"score = -log P(y|posterior)\"]\n        UNCERT[\"Â± incertidumbre<br/>(std, CI)\"]\n        CALC --> UNCERT\n    end\n\n    subgraph TRIAGE[\"ğŸš¨ 5. TRIAGE\"]\n        RISK[\"Risk = 0.5Ã—score<br/>+ 0.3Ã—confianza<br/>+ 0.2Ã—novedad\"]\n        ALERT[\"Alertas<br/>priorizadas\"]\n        RISK --> ALERT\n    end\n\n    DATA ==> FEATURES ==> MODEL ==>|\"MCMC<br/>2000 samples\"| SCORE ==> TRIAGE\n\n    style MODEL fill:#fff3e0\n    style SCORE fill:#fce4ec\n    style TRIAGE fill:#fff8e1\n```\n\n### Flujo en Una LÃ­nea\n\n```\nEventos â†’ Agregar por entidad â†’ Modelo Bayesiano â†’ Score (-log P) â†’ Risk Score â†’ Alertas TOP-K\n```\n\n### Variables Clave en Cada Etapa\n\n| Etapa | Variables | Dimensiones |\n|-------|-----------|-------------|\n| **Input** | `event_count`, `entity_id`, `timestamp` | N observaciones |\n| **Model** | `y[n]`, `entity_idx[n]`, `n_entities` | Arrays para PyMC |\n| **Posterior** | `Î¸[e]` (tasa), `Ï†` (dispersiÃ³n), `Î¼`, `Î±` | S muestras Ã— E entidades |\n| **Score** | `anomaly_score`, `score_std`, `CI_lower`, `CI_upper` | N observaciones |\n| **Risk** | `risk_score`, `rank`, `threshold` | N observaciones |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Pipeline End-to-End <a id='4-pipeline'></a>\n",
    "\n",
    "### Flujo de Datos\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Raw Events     â”‚  (timestamp, user_id, ip, endpoint, status, bytes...)\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼  build_features()\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  modeling_df    â”‚  Agregado por (entity, window): event_count, unique_ips...\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼  get_model_arrays()\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  arrays dict    â”‚  {y: ndarray, entity_idx: ndarray, n_entities: int}\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼  train_model()  [MCMC - LENTO]\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  trace          â”‚  ArviZ InferenceData con posteriors de Î¼, Î±, Î¸[e], Ï†\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼  compute_scores()\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  scores dict    â”‚  {anomaly_score, score_std, score_lower, score_upper}\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼  create_scored_df()\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  scored_df      â”‚  DataFrame con scores, ranks, intervalos, ground truth\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼  evaluate()\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  metrics        â”‚  {pr_auc, roc_auc, precision_at_k, recall_at_k...}\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Pasos del Pipeline en CÃ³digo\n",
    "\n",
    "| Paso | FunciÃ³n | Input | Output |\n",
    "|------|---------|-------|--------|\n",
    "| 1 | `generate_data()` | Settings | (events_df, attacks_df) |\n",
    "| 2 | `build_features()` | events_df | (modeling_df, metadata) |\n",
    "| 3 | `get_model_arrays()` | modeling_df | dict {y, entity_idx, ...} |\n",
    "| 4 | `train_model()` | arrays, settings | trace (InferenceData) |\n",
    "| 5 | `compute_scores()` | y, trace, entity_idx | scores dict |\n",
    "| 6 | `compute_intervals()` | trace, entity_idx | intervals dict |\n",
    "| 7 | `create_scored_df()` | modeling_df, scores | scored_df |\n",
    "| 8 | `evaluate()` | scored_df | metrics dict |\n",
    "| 9 | `create_plots()` | scored_df, metrics | plot paths |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. MÃ©tricas y EvaluaciÃ³n <a id='5-mÃ©tricas'></a>\n",
    "\n",
    "### MÃ©tricas de ClasificaciÃ³n\n",
    "\n",
    "| MÃ©trica | FÃ³rmula | CuÃ¡ndo Usar |\n",
    "|---------|---------|-------------|\n",
    "| **PR-AUC** | Ãrea bajo Precision-Recall curve | **PRIMARIA** para eventos raros |\n",
    "| **ROC-AUC** | Ãrea bajo ROC curve | Secundaria; puede ser engaÃ±osa con imbalance |\n",
    "| **Precision@k** | TP en top-k / k | \"De mis top 50 alertas, Â¿cuÃ¡ntas son reales?\" |\n",
    "| **Recall@k** | TP en top-k / total_positivos | \"Â¿QuÃ© % de ataques capturo en top 50?\" |\n",
    "\n",
    "### Â¿Por quÃ© PR-AUC > ROC-AUC para eventos raros?\n",
    "\n",
    "```\n",
    "Ejemplo con 2% attack rate (98 normales, 2 ataques):\n",
    "\n",
    "Modelo \"predice todo normal\":\n",
    "â”œâ”€â”€ ROC-AUC: Puede ser ~0.50 (parece \"random\")\n",
    "â””â”€â”€ PR-AUC: ~0.02 (refleja el fracaso real)\n",
    "\n",
    "Modelo mediocre que detecta 1 de 2 ataques con 10 FP:\n",
    "â”œâ”€â”€ ROC-AUC: ~0.95 (parece excelente!)\n",
    "â”‚   â””â”€â”€ Porque TNR = 88/98 = 0.90 domina\n",
    "â””â”€â”€ PR-AUC: ~0.15 (refleja la realidad operacional)\n",
    "    â””â”€â”€ Precision = 1/11 = 0.09\n",
    "\n",
    "CONCLUSIÃ“N: PR-AUC es mÃ¡s honesto con class imbalance.\n",
    "```\n",
    "\n",
    "### MÃ©tricas MCMC (DiagnÃ³sticos)\n",
    "\n",
    "| MÃ©trica | Valor OK | InterpretaciÃ³n |\n",
    "|---------|----------|----------------|\n",
    "| **R-hat** | < 1.05 | Convergencia entre cadenas |\n",
    "| **ESS (bulk)** | > 400 | Muestras efectivas independientes |\n",
    "| **Divergences** | 0 | Problemas de geometrÃ­a del posterior |\n",
    "\n",
    "```python\n",
    "# CÃ³mo verificar en cÃ³digo:\n",
    "import arviz as az\n",
    "summary = az.summary(trace, var_names=[\"mu\", \"alpha\", \"phi\"])\n",
    "print(summary[[\"r_hat\", \"ess_bulk\"]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. MÃ³dulo Triage (Operacional) <a id='6-triage'></a>\n",
    "\n",
    "### Componentes\n",
    "\n",
    "```\n",
    "src/triage/\n",
    "â”œâ”€â”€ risk_score.py           # Score compuesto\n",
    "â”œâ”€â”€ calibrate_thresholds.py # CalibraciÃ³n de alertas\n",
    "â”œâ”€â”€ ranking_metrics.py      # MÃ©tricas operacionales\n",
    "â””â”€â”€ entity_context.py       # Enriquecimiento de alertas\n",
    "```\n",
    "\n",
    "### 1. RiskScorer: Score Compuesto\n",
    "\n",
    "```python\n",
    "risk = 0.5 * normalized_score  +  # Magnitud del score\n",
    "       0.3 * confidence        +  # 1/(1+std) - inverso de incertidumbre\n",
    "       0.2 * novelty              # Entidades nuevas = mÃ¡s riesgo\n",
    "```\n",
    "\n",
    "**Por quÃ©:** El score raw no es suficiente. Una anomalÃ­a con alta incertidumbre es menos confiable.\n",
    "\n",
    "### 2. AlertBudget: CalibraciÃ³n de Threshold\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  MODO                 PREGUNTA DEL SOC                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  fixed_alerts    \"Solo puedo manejar 50 alertas/dÃ­a\"     â”‚\n",
    "â”‚                  â†’ Calcula threshold para generar â‰¤50     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  fixed_recall    \"Quiero detectar al menos 30% de        â”‚\n",
    "â”‚                   ataques\"                                â”‚\n",
    "â”‚                  â†’ Calcula threshold para recall â‰¥ 0.30   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  fixed_fpr       \"Tolero mÃ¡ximo 5% de falsos positivos\"  â”‚\n",
    "â”‚                  â†’ Calcula threshold para FPR â‰¤ 0.05      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### 3. EntityContext: Enriquecimiento\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  ALERTA ENRIQUECIDA para el analista:                      â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚  Entity: user_0234                                         â”‚\n",
    "â”‚  Current value: 127 events (baseline: 45 Â± 12)             â”‚\n",
    "â”‚  Deviation: 6.8Ïƒ above baseline (extremely high)           â”‚\n",
    "â”‚  Global percentile: 99.2%                                  â”‚\n",
    "â”‚  Previous alerts for this entity: 3                        â”‚\n",
    "â”‚  Confidence: HIGH (narrow uncertainty bounds)              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Preguntas de Entrevista <a id='7-preguntas'></a>\n",
    "\n",
    "### Fundamentos del Modelo\n",
    "\n",
    "---\n",
    "\n",
    "**Q1: Â¿Por quÃ© Negative Binomial en lugar de Poisson?**\n",
    "\n",
    "> Poisson asume que varianza = media. En datos de seguridad, la varianza tÃ­picamente **excede** la media (overdispersion). \n",
    "> \n",
    "> Negative Binomial tiene un parÃ¡metro extra (Ï†) que modela esta dispersiÃ³n adicional:\n",
    "> - `Var(y) = Î¼ + Î¼Â²/Ï†`\n",
    "> - Cuando Ï† â†’ âˆ, NegBin â†’ Poisson\n",
    "> - Valores finitos de Ï† capturan overdispersion\n",
    "\n",
    "---\n",
    "\n",
    "**Q2: Â¿QuÃ© es partial pooling y por quÃ© es importante?**\n",
    "\n",
    "> Es un balance entre dos extremos:\n",
    "> - **No pooling**: Cada entidad se estima independientemente â†’ alta varianza para entidades con pocos datos\n",
    "> - **Complete pooling**: Todas las entidades comparten el mismo parÃ¡metro â†’ sesgo, ignora diferencias\n",
    ">\n",
    "> **Partial pooling** (lo que hace BSAD):\n",
    "> - Entidades con **pocos datos** â†’ su Î¸[e] se \"encoge\" hacia la media poblacional Î¼\n",
    "> - Entidades con **muchos datos** â†’ su Î¸[e] se estima principalmente de sus propios datos\n",
    "> - El modelo **aprende automÃ¡ticamente** cuÃ¡nto confiar en cada fuente\n",
    "\n",
    "---\n",
    "\n",
    "**Q3: Â¿Por quÃ© PR-AUC sobre ROC-AUC para eventos raros?**\n",
    "\n",
    "> Con 2% attack rate:\n",
    "> - ROC-AUC puede ser 0.95+ con un modelo mediocre porque **True Negatives dominan**\n",
    "> - PR-AUC **penaliza los falsos positivos** mÃ¡s severamente\n",
    "> - PR-AUC refleja mejor la realidad operacional: \"Â¿cuÃ¡ntas de mis alertas son reales?\"\n",
    "\n",
    "---\n",
    "\n",
    "**Q4: Â¿CÃ³mo se calcula el anomaly score?**\n",
    "\n",
    "> ```\n",
    "> score = -log P(y | posterior)\n",
    "> ```\n",
    "> \n",
    "> Pasos:\n",
    "> 1. Para cada muestra MCMC s, calcular P(y | Î¸_s, Ï†_s) usando la PMF de NegBinomial\n",
    "> 2. Promediar en log-space: `logsumexp(log_probs) - log(n_samples)`\n",
    "> 3. Negar: score alto = probabilidad baja = anÃ³malo\n",
    "\n",
    "---\n",
    "\n",
    "### DiagnÃ³sticos y Convergencia\n",
    "\n",
    "---\n",
    "\n",
    "**Q5: Â¿QuÃ© indica R-hat y cuÃ¡l es un valor aceptable?**\n",
    "\n",
    "> R-hat (Gelman-Rubin statistic) mide si **mÃºltiples cadenas MCMC convergieron** a la misma distribuciÃ³n.\n",
    "> - Compara varianza **within-chain** vs **between-chain**\n",
    "> - R-hat â‰ˆ 1.0 â†’ cadenas estÃ¡n mezcladas, convergieron\n",
    "> - **Aceptable: < 1.05** (algunos usan < 1.01)\n",
    "> - R-hat > 1.1 â†’ problema serio, no confiar en resultados\n",
    "\n",
    "---\n",
    "\n",
    "**Q6: Â¿QuÃ© es ESS y por quÃ© importa?**\n",
    "\n",
    "> **Effective Sample Size**: MCMC genera muestras **autocorrelacionadas**. ESS estima cuÃ¡ntas muestras **independientes** equivalentes tienes.\n",
    "> \n",
    "> - Si tienes 2000 muestras pero ESS = 200, es como tener solo 200 muestras independientes\n",
    "> - **MÃ­nimo recomendado: 400** para estimaciones estables\n",
    "> - ESS bajo indica: necesitas mÃ¡s samples, o el modelo tiene problemas de muestreo\n",
    "\n",
    "---\n",
    "\n",
    "**Q7: Â¿QuÃ© son las divergencias en HMC?**\n",
    "\n",
    "> Son **fallos numÃ©ricos** durante el muestreo Hamiltoniano.\n",
    "> - HMC simula una partÃ­cula moviÃ©ndose en el espacio de parÃ¡metros\n",
    "> - Divergencia = la simulaciÃ³n \"explota\" (error de integraciÃ³n)\n",
    "> - Indica **geometrÃ­a problemÃ¡tica** del posterior (curvaturas extremas)\n",
    "> - **DeberÃ­an ser 0**. Si hay muchas â†’ reparametrizar modelo o ajustar sampler\n",
    "\n",
    "---\n",
    "\n",
    "### DiseÃ±o y Trade-offs\n",
    "\n",
    "---\n",
    "\n",
    "**Q8: Â¿CÃ³mo maneja el modelo entidades nuevas (cold start)?**\n",
    "\n",
    "> Gracias al **partial pooling**:\n",
    "> - Entidad nueva â†’ prior muy fuerte hacia Î¼ (media poblacional)\n",
    "> - Primeras observaciones â†’ Î¸[e] cercano a Î¼\n",
    "> - A medida que acumula datos â†’ Î¸[e] se individualiza\n",
    "> \n",
    "> No hay \"cold start\" catastrÃ³fico porque siempre hay un prior informativo.\n",
    "\n",
    "---\n",
    "\n",
    "**Q9: Â¿Por quÃ© no usar Random Forest o XGBoost directamente?**\n",
    "\n",
    "> | Aspecto | Classifiers | BSAD |\n",
    "> |---------|-------------|------|\n",
    "> | Incertidumbre | No (o requiere calibraciÃ³n extra) | SÃ­, intervalos credibles |\n",
    "> | Class imbalance | Requiere SMOTE, class weights | Manejo natural |\n",
    "> | Estructura entidad | Feature engineering manual | Modelado explÃ­cito |\n",
    "> | Sparse entities | Problemas | Partial pooling |\n",
    "> | Interpretabilidad | Caja negra | ParÃ¡metros interpretables |\n",
    "\n",
    "---\n",
    "\n",
    "**Q10: Â¿CÃ³mo escala este enfoque?**\n",
    "\n",
    "> **Entrenamiento**: O(n_samples Ã— n_observations Ã— n_entities)\n",
    "> - LENTO (horas para modelos realistas)\n",
    "> - Se hace **offline**, no en tiempo real\n",
    "> \n",
    "> **Scoring**: O(n_observations Ã— n_posterior_samples)\n",
    "> - RÃPIDO (lookup + aritmÃ©tica)\n",
    "> - Apto para batch o near-real-time\n",
    "> \n",
    "> **Estrategia de producciÃ³n**:\n",
    "> - Reentrenar modelo diario/semanal\n",
    "> - Usar modelo entrenado para scoring continuo\n",
    "\n",
    "---\n",
    "\n",
    "### Operacional\n",
    "\n",
    "---\n",
    "\n",
    "**Q11: Â¿CÃ³mo calibras el threshold para alertas?**\n",
    "\n",
    "> Tres estrategias en `AlertBudget`:\n",
    "> 1. **Fixed alerts**: \"Puedo manejar 50 alertas/dÃ­a\" â†’ encuentra threshold\n",
    "> 2. **Fixed recall**: \"Quiero detectar 30% de ataques\" â†’ encuentra threshold\n",
    "> 3. **Fixed FPR**: \"Tolero 5% false positive rate\" â†’ encuentra threshold\n",
    "> \n",
    "> Cada modo tiene trade-offs diferentes entre workload y coverage.\n",
    "\n",
    "---\n",
    "\n",
    "**Q12: Â¿QuÃ© tipos de ataques puede detectar BSAD?**\n",
    "\n",
    "> **Detecta bien:**\n",
    "> - Brute force (conteo anÃ³malo de intentos)\n",
    "> - Credential stuffing (patrones de mÃºltiples usuarios)\n",
    "> - Data exfiltration (volumen de bytes inusual)\n",
    "> - Beaconing/C2 (patrones periÃ³dicos)\n",
    "> \n",
    "> **NO detecta:**\n",
    "> - Ataques de contenido (SQLi, XSS) - no modela contenido\n",
    "> - Zero-days que no afectan conteos\n",
    "> - Ataques \"low and slow\" que se mantienen bajo el radar\n",
    "\n",
    "---\n",
    "\n",
    "**Q13: Â¿CuÃ¡les son las limitaciones principales?**\n",
    "\n",
    "> 1. **Solo datos de conteo** - no features continuos multivariados\n",
    "> 2. **Requiere estructura entidad-tiempo** - no apto para datos sin entidades\n",
    "> 3. **Entrenamiento lento** - MCMC toma horas\n",
    "> 4. **No real-time** - latencia > 100ms\n",
    "> 5. **Requiere ground truth** para calibrar thresholds (aunque no para entrenar)\n",
    "\n",
    "---\n",
    "\n",
    "**Q14: Â¿CÃ³mo explicarÃ­as una alerta a un analista?**\n",
    "\n",
    "> \"El usuario X generÃ³ 127 eventos en esta ventana. Su baseline histÃ³rico es 45Â±12 eventos. \n",
    "> Esto es 6.8 desviaciones estÃ¡ndar por encima de lo normal para este usuario especÃ­fico.\n",
    "> El modelo asigna una probabilidad muy baja a este comportamiento, lo que resulta en un \n",
    "> anomaly score en el percentil 99.2. AdemÃ¡s, este usuario ha generado 3 alertas previas.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Q15: Â¿QuÃ© harÃ­as si el modelo tiene muchas divergencias?**\n",
    "\n",
    "> 1. **Aumentar target_accept** (ej: 0.95 â†’ 0.99) - pasos mÃ¡s pequeÃ±os\n",
    "> 2. **Reparametrizar** - usar non-centered parameterization para Î¸[e]\n",
    "> 3. **Verificar priors** - priors muy amplios pueden causar problemas\n",
    "> 4. **Revisar datos** - outliers extremos pueden distorsionar el posterior\n",
    "> 5. **Simplificar modelo** - quizÃ¡s demasiados parÃ¡metros para los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 7.5 Benchmark y Robustez (NUEVO) <a id='7.5-benchmark'></a>\n\n### Benchmark Reproducible\n\nBSAD se compara contra baselines especÃ­ficos para datos de conteo:\n\n| Modelo | PR-AUC | ROC-AUC | Recall@50 | Tipo |\n|--------|--------|---------|-----------|------|\n| **BSAD** | 0.562 | 0.943 | 1.000 | Bayesiano JerÃ¡rquico |\n| NB_EmpBayes | 0.568 | 0.954 | 1.000 | EspecÃ­fico conteos |\n| GLMM_NB | 0.567 | 0.952 | 1.000 | EspecÃ­fico conteos |\n| NB_MLE | 0.466 | 0.856 | 0.800 | EspecÃ­fico conteos |\n\n**Insight**: BSAD y NB_EmpBayes rinden similar porque ambos usan partial pooling. La ventaja de BSAD es la cuantificaciÃ³n completa de incertidumbre.\n\n### AnÃ¡lisis de Robustez\n\n| Test | Resultado | ImplicaciÃ³n |\n|------|-----------|-------------|\n| **Sensibilidad Tasa Ataque** | PR-AUC: 0.46 (0.5%) â†’ 0.89 (10%) | DiseÃ±ado para <5% |\n| **Deriva Temporal** | Sin degradaciÃ³n (+7.7% en test) | Seguro para producciÃ³n |\n| **Arranque en FrÃ­o** | -14% para entidades nuevas | Aceptable con partial pooling |\n| **Estabilidad Ranking** | Spearman Ï=0.51 | Reentrenamiento semanal |\n\n### Preguntas de Entrevista sobre Benchmark\n\n---\n\n**Q16: Â¿CÃ³mo validaste que tu modelo funciona mejor que las alternativas?**\n\n> CreÃ© un benchmark reproducible comparando BSAD contra:\n> 1. **Baselines justos** (especÃ­ficos para conteos): NB_MLE, NB_EmpBayes, GLMM_NB\n> 2. **Detectores genÃ©ricos** (referencia): Isolation Forest, One-Class SVM, LOF\n>\n> Resultado: BSAD tiene rendimiento similar a NB_EmpBayes (~0.56 PR-AUC), lo cual es esperado porque ambos usan partial pooling. La ventaja de BSAD es la **cuantificaciÃ³n de incertidumbre completa**.\n\n---\n\n**Q17: Â¿CÃ³mo probaste la robustez del modelo?**\n\n> EjecutÃ© 4 tests de robustez:\n> 1. **Sensibilidad a tasa de ataque**: PR-AUC escala de 0.46 (0.5%) a 0.89 (10%)\n> 2. **Deriva temporal**: Sin degradaciÃ³n en datos futuros (+7.7% en test vs train)\n> 3. **Arranque en frÃ­o**: -14% para entidades no vistas (aceptable con partial pooling)\n> 4. **Estabilidad de ranking**: Spearman Ï=0.51 (moderada, sugiere reentrenamiento semanal)\n\n---\n\n**Q18: Â¿Por quÃ© BSAD no gana siempre en ROC-AUC?**\n\n> Porque estamos comparando manzanas con naranjas:\n> - **Random Forest** usa labels (supervisado)\n> - **BSAD** es no supervisado\n>\n> La comparaciÃ³n justa es en **mÃ©tricas operacionales**: BSAD genera 8-14Ã— menos alertas que RF para el mismo recall. Eso es lo que importa en un SOC.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. CÃ³digo Ejecutable <a id='8-cÃ³digo'></a>\n",
    "\n",
    "Ejecuta las siguientes celdas para ver el modelo en acciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from bsad.config import Settings\n",
    "from bsad import steps\n",
    "\n",
    "print(\"âœ“ Imports OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfiguraciÃ³n para demo rÃ¡pido\n",
    "settings = Settings(\n",
    "    n_entities=50,      # Pocas entidades para demo\n",
    "    n_days=14,          # 2 semanas\n",
    "    attack_rate=0.03,   # 3% ataques\n",
    "    n_samples=500,      # Pocas muestras MCMC (solo demo)\n",
    "    n_tune=300,\n",
    "    n_chains=2,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"ConfiguraciÃ³n:\")\n",
    "print(f\"  - Entidades: {settings.n_entities}\")\n",
    "print(f\"  - DÃ­as: {settings.n_days}\")\n",
    "print(f\"  - Attack rate: {settings.attack_rate:.1%}\")\n",
    "print(f\"  - MCMC samples: {settings.n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Generar datos sintÃ©ticos\n",
    "print(\"Generando datos sintÃ©ticos...\")\n",
    "events_df, attacks_df = steps.generate_data(settings)\n",
    "\n",
    "print(f\"\\nâœ“ Eventos generados: {len(events_df):,}\")\n",
    "print(f\"âœ“ Ataques inyectados: {len(attacks_df)}\")\n",
    "print(f\"\\nTipos de ataque:\")\n",
    "print(attacks_df['attack_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2: Feature engineering\n",
    "print(\"Construyendo features...\")\n",
    "modeling_df, metadata = steps.build_features(events_df, attacks_df, settings)\n",
    "\n",
    "print(f\"\\nâœ“ Observaciones (entity-windows): {len(modeling_df):,}\")\n",
    "print(f\"âœ“ Entidades Ãºnicas: {metadata['n_entities']}\")\n",
    "print(f\"âœ“ Attack rate: {metadata['attack_rate']:.2%}\")\n",
    "\n",
    "print(f\"\\nColumnas del modeling_df:\")\n",
    "print(modeling_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribuciÃ³n de conteos (overdispersion)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Histograma de event_count\n",
    "axes[0].hist(modeling_df['event_count'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(modeling_df['event_count'].mean(), color='red', linestyle='--', label=f\"Mean: {modeling_df['event_count'].mean():.1f}\")\n",
    "axes[0].axvline(modeling_df['event_count'].std(), color='orange', linestyle=':', label=f\"Std: {modeling_df['event_count'].std():.1f}\")\n",
    "axes[0].set_xlabel('Event Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('DistribuciÃ³n de Conteos')\n",
    "axes[0].legend()\n",
    "\n",
    "# Overdispersion check\n",
    "mean_count = modeling_df['event_count'].mean()\n",
    "var_count = modeling_df['event_count'].var()\n",
    "axes[1].bar(['Mean', 'Variance'], [mean_count, var_count], color=['steelblue', 'coral'])\n",
    "axes[1].set_title(f'Overdispersion Check\\nVar/Mean = {var_count/mean_count:.2f}')\n",
    "axes[1].set_ylabel('Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š Overdispersion ratio (Var/Mean): {var_count/mean_count:.2f}\")\n",
    "print(f\"   Si fuera Poisson, este ratio serÃ­a â‰ˆ 1.0\")\n",
    "print(f\"   Ratio > 1 indica overdispersion â†’ justifica Negative Binomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Extraer arrays para PyMC\n",
    "arrays = steps.get_model_arrays(modeling_df)\n",
    "\n",
    "print(\"Arrays para el modelo:\")\n",
    "for key, val in arrays.items():\n",
    "    if isinstance(val, np.ndarray):\n",
    "        print(f\"  {key}: shape={val.shape}, dtype={val.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Entrenar modelo (MCMC)\n",
    "print(\"Entrenando modelo Bayesiano...\")\n",
    "print(\"(Esto puede tomar 1-2 minutos con la configuraciÃ³n de demo)\\n\")\n",
    "\n",
    "trace = steps.train_model(arrays, settings)\n",
    "\n",
    "print(\"\\nâœ“ Modelo entrenado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DiagnÃ³sticos MCMC\n",
    "import arviz as az\n",
    "\n",
    "print(\"ğŸ“Š DiagnÃ³sticos MCMC:\\n\")\n",
    "summary = az.summary(trace, var_names=[\"mu\", \"alpha\", \"phi\"])\n",
    "print(summary[[\"mean\", \"sd\", \"r_hat\", \"ess_bulk\"]].round(3))\n",
    "\n",
    "# Verificar convergencia\n",
    "r_hats = summary['r_hat'].values\n",
    "converged = all(r < 1.05 for r in r_hats)\n",
    "print(f\"\\n{'âœ“' if converged else 'âœ—'} Convergencia: {'OK' if converged else 'PROBLEMA'} (todos R-hat < 1.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5-7: Scoring\n",
    "print(\"Calculando anomaly scores...\")\n",
    "\n",
    "scores = steps.compute_scores(arrays['y'], trace, arrays['entity_idx'])\n",
    "intervals = steps.compute_intervals(trace, arrays['entity_idx'])\n",
    "scored_df = steps.create_scored_df(modeling_df, scores, intervals)\n",
    "\n",
    "print(f\"\\nâœ“ Scored {len(scored_df)} observaciones\")\n",
    "print(f\"\\nTop 10 anomalÃ­as:\")\n",
    "print(scored_df[['entity_id', 'event_count', 'anomaly_score', 'has_attack']].head(10).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 8: EvaluaciÃ³n\n",
    "metrics = steps.evaluate(scored_df)\n",
    "\n",
    "print(\"ğŸ“ˆ MÃ©tricas de EvaluaciÃ³n:\\n\")\n",
    "print(f\"  PR-AUC:  {metrics['pr_auc']:.3f}\")\n",
    "print(f\"  ROC-AUC: {metrics['roc_auc']:.3f}\")\n",
    "print(f\"\\n  Precision@10: {metrics.get('precision_at_10', 'N/A')}\")\n",
    "print(f\"  Precision@25: {metrics.get('precision_at_25', 'N/A')}\")\n",
    "print(f\"  Recall@10:    {metrics.get('recall_at_10', 'N/A')}\")\n",
    "print(f\"  Recall@25:    {metrics.get('recall_at_25', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaciÃ³n: Score Distribution por clase\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Score distribution\n",
    "for label, group in scored_df.groupby('has_attack'):\n",
    "    axes[0].hist(group['anomaly_score'], bins=30, alpha=0.6, \n",
    "                 label=f\"{'Attack' if label else 'Normal'}\", density=True)\n",
    "axes[0].set_xlabel('Anomaly Score')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('DistribuciÃ³n de Scores por Clase')\n",
    "axes[0].legend()\n",
    "\n",
    "# PR Curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "precision, recall, _ = precision_recall_curve(scored_df['has_attack'], scored_df['anomaly_score'])\n",
    "axes[1].plot(recall, precision, 'b-', linewidth=2, label=f'BSAD (PR-AUC={metrics[\"pr_auc\"]:.3f})')\n",
    "axes[1].axhline(y=scored_df['has_attack'].mean(), color='r', linestyle='--', label='Random baseline')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisualizaciÃ³n: Top anomalÃ­as con incertidumbre\n",
    "top_n = 15\n",
    "top_anomalies = scored_df.head(top_n).copy()\n",
    "top_anomalies['idx'] = range(top_n)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Barras de score con error bars\n",
    "colors = ['coral' if attack else 'steelblue' for attack in top_anomalies['has_attack']]\n",
    "ax.barh(top_anomalies['idx'], top_anomalies['anomaly_score'], \n",
    "        xerr=top_anomalies['score_std'], color=colors, alpha=0.7, capsize=3)\n",
    "\n",
    "# Labels\n",
    "ax.set_yticks(top_anomalies['idx'])\n",
    "ax.set_yticklabels([f\"{row['entity_id']} (count={row['event_count']})\" \n",
    "                    for _, row in top_anomalies.iterrows()])\n",
    "ax.set_xlabel('Anomaly Score')\n",
    "ax.set_title(f'Top {top_n} AnomalÃ­as con Intervalos de Incertidumbre')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='coral', label='Attack'),\n",
    "                   Patch(facecolor='steelblue', label='Normal')]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ¯ De los top {top_n}: {top_anomalies['has_attack'].sum()} son ataques reales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resumen para la Entrevista\n",
    "\n",
    "### Elevator Pitch (30 segundos)\n",
    "\n",
    "> \"BSAD es un sistema de detecciÃ³n de anomalÃ­as para datos de seguridad que usa modelado Bayesiano jerÃ¡rquico. \n",
    "> A diferencia de mÃ©todos clÃ¡sicos, BSAD aprende un baseline personalizado para cada entidad (usuario, IP, etc.) \n",
    "> mientras comparte informaciÃ³n entre entidades similares via partial pooling. Esto lo hace ideal para \n",
    "> detectar eventos raros como brute force o data exfiltration, donde el class imbalance es extremo \n",
    "> y necesitamos cuantificar la incertidumbre en nuestras predicciones.\"\n",
    "\n",
    "### Puntos Clave\n",
    "\n",
    "1. **Negative Binomial** para overdispersion (var > mean)\n",
    "2. **Partial pooling** para entidades sparse\n",
    "3. **PR-AUC** como mÃ©trica primaria (no ROC-AUC)\n",
    "4. **Incertidumbre cuantificada** via posterior samples\n",
    "5. **Triage module** para operacionalizaciÃ³n (calibraciÃ³n de thresholds, enriquecimiento de alertas)\n",
    "\n",
    "### Lo que NO hace BSAD\n",
    "\n",
    "- No es real-time (MCMC es lento)\n",
    "- No detecta ataques de contenido (SQLi, XSS)\n",
    "- No maneja features continuos multivariados\n",
    "- No funciona sin estructura entidad-tiempo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}